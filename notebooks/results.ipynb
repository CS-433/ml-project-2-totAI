{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and Results\n",
    "\n",
    "In this notebook we will be experimenting with different **versions** of our models, **evaluating** their performances and **comparing** the results.\n",
    "\n",
    "We chose two model architectures: `U-Net`and `DeepLabv3`.\n",
    "\n",
    "The notebook will have the following structure: \n",
    "1. Training `U-Net` on unprocessed data\n",
    "2. Training `U-Net` on processed data where we only keep the roofs on images \n",
    "3. Training `U-Net` on processed data where we only keep the roofs and augment the data \n",
    "4. Training `DeepLabv3` on processed data where we only keep the roofs on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-09T16:26:57.480928Z",
     "iopub.status.busy": "2024-12-09T16:26:57.480111Z",
     "iopub.status.idle": "2024-12-09T16:26:57.485982Z",
     "shell.execute_reply": "2024-12-09T16:26:57.485046Z",
     "shell.execute_reply.started": "2024-12-09T16:26:57.480895Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import albumentations as A\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tabulate import tabulate\n",
    "\n",
    "src_path = Path('../src').resolve()  \n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "\n",
    "from dataset import get_loaders\n",
    "from model_functions import predict, evaluate\n",
    "from plotting import plot_prediction, plot_losses, plot_model_metrics\n",
    "\n",
    "from models.unet.architecture import UNet\n",
    "from models.unet.training import train_unet\n",
    "from models.unet.hyperparameters import optimize_threshold, optimize_unet_hyperparameters\n",
    "\n",
    "from models.deeplab.architecture import Deeplabv3Plus\n",
    "from models.deeplab.training import train_deeplab\n",
    "from models.deeplab.utils import DiceBCELoss, IoU\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data\n",
    "\n",
    "Let's start by importing the data, you can start by downloading our dataset from [Kaggle](https://www.kaggle.com/datasets/jeanperbet/ml-project-2-solar-panels?select=roof_images) and adding it to the working directory under the folder name `data`.\n",
    "\n",
    "We will consider three sets of data:\n",
    "- **Original images** : unmodified 1000 $\\times$ 1000 pixel tiles.\n",
    "- **Roof-only images** : processed 1000 $\\times$ 1000 pixel tiles where all non-roof pixels were colored in *magenta*.\n",
    "- **Labels** : grayscale 1000 $\\times$ 1000 pixel tiles encoding the positions of the solar panels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"../data/\"\n",
    "images_dir = os.path.join(ROOT_DIR, \"images/\")\n",
    "roof_images_dir = os.path.join(ROOT_DIR, \"roofs/images/\")\n",
    "labels_dir = os.path.join(ROOT_DIR, \"labels/\")\n",
    "weights_dir = os.path.join(ROOT_DIR, \"weights/\")\n",
    "roof_masks_dir = os.path.join(ROOT_DIR, \"roofs/masks/\")\n",
    "\n",
    "image_names = [file.replace(\".jpg\", \"\") for file in os.listdir(images_dir) if file.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data \n",
    "\n",
    "In order to evaluate our model well, we will be segmenting our dataset into three subsets: **train set**, **validation set** and **test set**.  Both trainn and validation sets will be used during the training and the test set will be use dofr the final evaluation of the model in order to compute the performance metrics. \n",
    "\n",
    "We chose the following split: $80\\%$ for train, $10\\%$ for validation and $10\\%$ for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:26:58.137723Z",
     "iopub.status.busy": "2024-12-09T16:26:58.137395Z",
     "iopub.status.idle": "2024-12-09T16:26:58.143335Z",
     "shell.execute_reply": "2024-12-09T16:26:58.142438Z",
     "shell.execute_reply.started": "2024-12-09T16:26:58.137694Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════════════╤════════════════════╕\n",
      "│ Dataset Split   │   Number of Images │\n",
      "╞═════════════════╪════════════════════╡\n",
      "│ Train           │                  8 │\n",
      "├─────────────────┼────────────────────┤\n",
      "│ Validation      │                  2 │\n",
      "├─────────────────┼────────────────────┤\n",
      "│ Test            │                  2 │\n",
      "╘═════════════════╧════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "train_transform = ToTensorV2()\n",
    "val_test_transform = ToTensorV2()\n",
    "batch_size = 2\n",
    "\n",
    "images_train_loader, images_val_loader, images_test_loader = get_loaders(\n",
    "    image_names=image_names[:10],\n",
    "    images_dir=images_dir, \n",
    "    labels_dir=labels_dir, \n",
    "    batch_size=batch_size, \n",
    "    train_transform=train_transform, \n",
    "    val_test_transform=val_test_transform\n",
    ")\n",
    "roof_images_train_loader, roof_images_val_loader, roof_images_test_loader = get_loaders(\n",
    "    image_names=image_names[:10], \n",
    "    images_dir=roof_images_dir, \n",
    "    labels_dir=labels_dir, \n",
    "    batch_size=batch_size,\n",
    "    train_transform=train_transform,\n",
    "    val_test_transform=val_test_transform\n",
    ")\n",
    "table_data = [[\"Train\", len(images_train_loader) * batch_size],\n",
    "              [\"Validation\", len(images_val_loader) * batch_size],\n",
    "              [\"Test\", len(images_test_loader) * batch_size]]\n",
    "print(tabulate(table_data, headers=[\"Dataset Split\", \"Number of Images\"], tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models\n",
    "\n",
    "> In order to accelerate our training, we will be using **early stopping**, which is a regularization technique that halts training when the validation performance stops improving, preventing overfitting to the training data. It monitors the loss after each epoch and stops training if no improvement is observed for a specified number of epochs **(patience)** that we set to $3$ by default. This approach is particularly useful as it balances training duration with model performance.\n",
    "\n",
    "> We will use **Binary Cross-Entropy Loss** as our loss function, since it performs well for binary classification tasks like the one we're tackling.\n",
    "\n",
    "To ensure that the models are trained on the best available architecture, we eventually move the computations to a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:26:58.938983Z",
     "iopub.status.busy": "2024-12-09T16:26:58.938645Z",
     "iopub.status.idle": "2024-12-09T16:26:58.943845Z",
     "shell.execute_reply": "2024-12-09T16:26:58.942902Z",
     "shell.execute_reply.started": "2024-12-09T16:26:58.938954Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "patience = 3\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step n°1 - Training `U-Net` on regular images\n",
    "\n",
    "We will start experimenting with the \"raw\" images, obtained during preprocessing (see notebook `preprocessing.ipynb`).\n",
    "\n",
    "Before going any further, we need to figure out the best values for the hyperparameters we will use during training. We will compute them once, using `optuna` library. Even if we will add some changes in later steps (i.e. augmenting the data, etc.), we assume that the hyperparameters we'll find now for U-Net on regular images will also be a good fit for later steps, since we will re-use the same model. We now that a further improvement could be fine-tuning hyperparameters each time we modify the training pipeline, but given our time and resources we take this small liberty.\n",
    "\n",
    "In the detail, we will optimize hyperparameters based on the validation loss for each epoch, i.e. the averaged loss for each data point of our validation set. We would like to find the best values for the **learning rate**, **the number of epochs**, the **kernel size** of our encoder and the **number of layers** in our U-Net encoder/decoder. The code is available in `src/hyperparameters.py`.\n",
    "\n",
    "> Due to the long running time of such an optimization, we comment out the hyperparameters optimization and already provide them. If you have some time though, you can uncomment the hyperparameters optimizaiton to have an idea of the hyperparameters importances ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/4 [00:03<?, ?it/s]\n",
      "[W 2024-12-16 18:14:44,542] Trial 0 failed with parameters: {'lr': 0.0024944618079534675, 'epochs': 1, 'encoder_layers': 2, 'kernel_size': 7} because of the following error: RuntimeError('MPS backend out of memory (MPS allocated: 9.01 GB, other allocations: 2.17 MB, max allowed: 9.07 GB). Tried to allocate 488.28 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).').\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jeanperbet/.virtualenvs/ml2/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/Users/jeanperbet/Projects/ML2/src/models/unet/hyperparameters.py\", line 73, in <lambda>\n",
      "    study = optuna.create_study(direction=\"minimize\")\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jeanperbet/Projects/ML2/src/models/unet/hyperparameters.py\", line 116, in unet_objective\n",
      "    image, label = image.to(device), label.to(device)\n",
      "              ^^^^^^^^^^^^\n",
      "  File \"/Users/jeanperbet/.virtualenvs/ml2/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jeanperbet/.virtualenvs/ml2/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jeanperbet/Projects/ML2/src/architectures/unet/model.py\", line 46, in forward\n",
      "  File \"/Users/jeanperbet/.virtualenvs/ml2/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jeanperbet/.virtualenvs/ml2/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jeanperbet/.virtualenvs/ml2/lib/python3.12/site-packages/torch/nn/modules/container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"/Users/jeanperbet/.virtualenvs/ml2/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jeanperbet/.virtualenvs/ml2/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jeanperbet/.virtualenvs/ml2/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jeanperbet/.virtualenvs/ml2/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "RuntimeError: MPS backend out of memory (MPS allocated: 9.01 GB, other allocations: 2.17 MB, max allowed: 9.07 GB). Tried to allocate 488.28 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "[W 2024-12-16 18:14:44,545] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 9.01 GB, other allocations: 2.17 MB, max allowed: 9.07 GB). Tried to allocate 488.28 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m nb_layers, kernel_size, lr, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3e-4\u001b[39m, \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m----> 2\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_unet_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages_val_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m nb_layers, kernel_size, lr, epochs \u001b[38;5;241m=\u001b[39m best_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m], best_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], best_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m], best_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Projects/ML2/src/models/unet/hyperparameters.py:73\u001b[0m, in \u001b[0;36moptimize_unet_hyperparameters\u001b[0;34m(train_loader, val_loader, n_trials, device)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize_unet_hyperparameters\u001b[39m(\n\u001b[1;32m     59\u001b[0m     train_loader: DataLoader, val_loader: DataLoader, n_trials: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, device: torch\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    Optimize hyperparameters of a U-Net model using Optuna, namely learning rate, number of\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    epochs, number of layers and kernel size.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m        dict: Best hyperparameters found by Optuna.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: unet_objective(trial, train_loader, val_loader, device), n_trials\u001b[38;5;241m=\u001b[39mn_trials)\n\u001b[1;32m     75\u001b[0m     fig \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mvisualization\u001b[38;5;241m.\u001b[39mplot_param_importances(study)\n",
      "File \u001b[0;32m~/.virtualenvs/ml2/lib/python3.12/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/ml2/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.virtualenvs/ml2/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.virtualenvs/ml2/lib/python3.12/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.virtualenvs/ml2/lib/python3.12/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m~/Projects/ML2/src/models/unet/hyperparameters.py:73\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize_unet_hyperparameters\u001b[39m(\n\u001b[1;32m     59\u001b[0m     train_loader: DataLoader, val_loader: DataLoader, n_trials: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, device: torch\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    Optimize hyperparameters of a U-Net model using Optuna, namely learning rate, number of\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    epochs, number of layers and kernel size.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m        dict: Best hyperparameters found by Optuna.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: unet_objective(trial, train_loader, val_loader, device), n_trials\u001b[38;5;241m=\u001b[39mn_trials)\n\u001b[1;32m     75\u001b[0m     fig \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mvisualization\u001b[38;5;241m.\u001b[39mplot_param_importances(study)\n",
      "File \u001b[0;32m~/Projects/ML2/src/models/unet/hyperparameters.py:116\u001b[0m, in \u001b[0;36munet_objective\u001b[0;34m(trial, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m    114\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, label, _ \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 116\u001b[0m     image, label \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device), label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    117\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(image)\n\u001b[1;32m    118\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, label)\n",
      "File \u001b[0;32m~/.virtualenvs/ml2/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/ml2/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/ML2/src/architectures/unet/model.py:46\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n",
      "File \u001b[0;32m~/.virtualenvs/ml2/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/ml2/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.virtualenvs/ml2/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/ml2/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/ml2/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.virtualenvs/ml2/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/ml2/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 9.01 GB, other allocations: 2.17 MB, max allowed: 9.07 GB). Tried to allocate 488.28 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "nb_layers, kernel_size, lr, epochs = 3, 5, 3e-4, 50\n",
    "# best_params = optimize_unet_hyperparameters(train_loader=images_train_loader, val_loader=images_val_loader, n_trials=2, device=device)\n",
    "# nb_layers, kernel_size, lr, epochs = best_params[\"encoder_layers\"], best_params[\"kernel_size\"], best_params[\"lr\"], best_params[\"epochs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train the model ! The following cell will use the previously-computed best hyperparameters to train a U-Net model on the images of our dataset.\n",
    "\n",
    "> Due to the long running time of such a training, we comment out the training phase and we already provide the weights for our pre-trained model, in the `data/weights` folder. If you have some time though, you can uncomment the training  code and run it yourself !   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:26:59.192818Z",
     "iopub.status.busy": "2024-12-09T16:26:59.191986Z",
     "iopub.status.idle": "2024-12-09T16:45:50.626178Z",
     "shell.execute_reply": "2024-12-09T16:45:50.625250Z",
     "shell.execute_reply.started": "2024-12-09T16:26:59.192780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "step_1_model = UNet(nb_layers=nb_layers, kernel_size=kernel_size)\n",
    "if device != \"cuda\":\n",
    "    step_1_model.load_state_dict(torch.load(os.path.join(weights_dir, \"unet_step_1_weights.pth\"), map_location=torch.device('cpu')))\n",
    "else:\n",
    "    step_1_model.load_state_dict(torch.load(os.path.join(weights_dir, \"unet_step_1_weights.pth\")))\n",
    "step_1_model.to(device);\n",
    "\n",
    "# step_1_train_losses, step_1_val_losses = train_model(\n",
    "#     model=step_1_model,\n",
    "#     train_loader=images_train_loader,\n",
    "#     val_loader=images_val_loader,\n",
    "#     device=device,\n",
    "#     criterion=nn.BCELoss(),\n",
    "#     lr=lr,\n",
    "#     epochs=epochs,\n",
    "#     patience=patience\n",
    "# )\n",
    "# plot_losses(step_1_train_losses, step_1_val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is trained, we have another important value to figure out. Indeed, our model outputs probabilities for each pixel of the input image of being part of a solar panel. We need to determine the most optimal probability **threshold** above which we classify a pixel as `solar_panel` or not. Following a similar reasoning as for hyperparameters, we will use `optuna` to determine the threshold maximizing both **IoU** and **F1-Score**.\n",
    "\n",
    "> Due to the long running time of the threshold optimization, we already provide it. But again, feel free to run the code yourself by uncommenting the relevant parts in the cell below !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_1_threshold = 0.12\n",
    "# step_1_threshold = optimize_threshold(model=step_1_model, test_loader=images_test_loader, n_trials=20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize some of the results. We will always use the same image throughout this notebook, in order to have a better understanding of the progress we make at each step. We'll use the index n°$0$ in our test loader for comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:45:50.850415Z",
     "iopub.status.busy": "2024-12-09T16:45:50.850177Z",
     "iopub.status.idle": "2024-12-09T16:45:51.644731Z",
     "shell.execute_reply": "2024-12-09T16:45:51.643854Z",
     "shell.execute_reply.started": "2024-12-09T16:45:50.850389Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "real_image, ground_truth, prediction = predict(\n",
    "    model=step_1_model, \n",
    "    test_loader=images_test_loader,\n",
    "    idx=test_idx, \n",
    "    threshold=step_1_threshold,\n",
    "    device=device\n",
    ")\n",
    "plot_prediction(real_image, ground_truth, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not so bad ! We can notice three interesting mistakes from the model, that we may be able to correct using some **post-processing**.\n",
    "\n",
    "1. Firstly, the output images contain some clusters of **noisy dots** that are **too small** to be real solar panels. We could try analysing the connected components in the model output to remove the ones below a certain threshold. \n",
    "\n",
    "2. Secondly, solar panels tend to have **straight edges**, whereas the model sometimes output messy non-regular edges. We could smooth the edges to try to match real-world patterns. \n",
    "\n",
    "3. Finally, some pixels outside of roofs (such as shadows for instance) are wrongly interpreted by the model as being solar panels. We have the necessary data (see `preprocessing.ipynb`) to **mask out non-roof areas**, that's why we could try setting all pixels that are outside of roofs in the model output to $0$, i.e. non-`solar_panel`.\n",
    "\n",
    "All the code to perform these post-processing operatins is available in `src/postprocessing.py`. Let's apply all these operations and visualize the results again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_image, ground_truth, prediction = predict(\n",
    "    model=step_1_model, \n",
    "    test_loader=images_test_loader,\n",
    "    idx=test_idx, \n",
    "    threshold=step_1_threshold,\n",
    "    post_process=True,\n",
    "    roof_masks_dir=roof_masks_dir,\n",
    "    device=device,\n",
    ")\n",
    "plot_prediction(real_image, ground_truth, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate our model more rigorously using the following metrics: **accuracy**, **IoU** and **F1-Score**. The former is the intersection over union (also known as [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index)), and is defined as follows.\n",
    "$$ \n",
    "\\text{IoU} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "The latter is defined as follows.\n",
    "$$ \n",
    "\\text{F1-Score} = \\frac{2}{(\\frac{1}{\\text{Precision}} + \\frac{1}{\\text{Recall}})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:45:51.646157Z",
     "iopub.status.busy": "2024-12-09T16:45:51.645886Z",
     "iopub.status.idle": "2024-12-09T16:45:51.654262Z",
     "shell.execute_reply": "2024-12-09T16:45:51.653394Z",
     "shell.execute_reply.started": "2024-12-09T16:45:51.646130Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "step_1_accuracy, step_1_f1, step_1_iou = (\n",
    "    evaluate(\n",
    "        model=step_1_model, \n",
    "        test_loader=images_test_loader, \n",
    "        threshold=step_1_threshold, \n",
    "        device=device\n",
    "    )\n",
    ")\n",
    "table_data = [[\"Accuracy\", step_1_accuracy],\n",
    "              [\"F1-Score\", step_1_f1],\n",
    "              [\"IoU\", step_1_iou]]\n",
    "print(tabulate(table_data, headers=[\"Metric\", \"Value\"], tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare these metrics with the same ones obtained when post-processing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_1_post_accuracy, step_1_post_f1, step_1_post_iou = (\n",
    "    evaluate(\n",
    "        model=step_1_model, \n",
    "        test_loader=images_test_loader, \n",
    "        threshold=step_1_threshold, \n",
    "        post_process=True,\n",
    "        roof_masks_dir=roof_masks_dir,\n",
    "        device=device\n",
    "    )\n",
    ")\n",
    "table_data = [[\"Accuracy\", step_1_post_accuracy],\n",
    "              [\"F1-Score\", step_1_post_f1],\n",
    "              [\"IoU\", step_1_post_iou]]\n",
    "print(tabulate(table_data, headers=[\"Metric\", \"Value\"], tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe minor improvements using post-processing. However, we see some potential into removing the non-roof pixels from the output of our model. In the next steps, we'll try a different approach by removing non-roof parts of images directly before training instead of during post-processing.\n",
    "\n",
    "### Step n°2 - Training `U-Net` on roof-only images\n",
    "\n",
    "In this second step, as stated above, we'll try to reduce the complexity of the data before feeding it the model. Our assumption is that **solar panels can only lie on roofs**. Hence, we decided to color every pixel that does not lie on a roof using **magenta** (see notebook `preprocessing.ipynb` for further explanations). This way, we make the model focus more on the roof-based pixels. The images we'll use from now on look like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(os.path.join(roof_images_dir, image_names[1] + \".jpg\")).resize((256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same reasoning as above, we provide the commented training code as well as the pre-trained model. Feel free to train the model yourself instead of loading the weights we provide !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_2_model = UNet(nb_layers=nb_layers, kernel_size=kernel_size)\n",
    "if device != \"cuda\":\n",
    "    step_2_model.load_state_dict(torch.load(os.path.join(weights_dir, \"unet_step_1_weights.pth\"), map_location=torch.device('cpu')))\n",
    "else:\n",
    "    step_2_model.load_state_dict(torch.load(os.path.join(weights_dir, \"unet_step_1_weights.pth\")))\n",
    "step_2_model.to(device);\n",
    "\n",
    "# step_2_train_losses, step_2_val_losses = train_model(\n",
    "#     model=step_2_model,\n",
    "#     train_loader=roof_images_train_loader,\n",
    "#     val_loader=roof_images_val_loader,\n",
    "#     device=device,\n",
    "#     criterion=nn.BCELoss(),\n",
    "#     lr=lr,\n",
    "#     epochs=epochs,\n",
    "#     patience=patience\n",
    "# )\n",
    "# plot_losses(step_2_train_losses, step_2_val_losses)\n",
    "\n",
    "step_2_threshold = 0.12\n",
    "# step_2_threshold = optimize_threshold(model=step_2_model, test_loader=roof_images_test_loader, n_trials=20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our results on the same image used for step n°1, to see if it yields an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_image, ground_truth, prediction = predict(\n",
    "    model=step_2_model, \n",
    "    test_loader=roof_images_test_loader,\n",
    "    idx=test_idx, \n",
    "    threshold=step_2_threshold,\n",
    "    device=device,\n",
    ")\n",
    "plot_prediction(real_image, ground_truth, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute more accurate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_2_accuracy, step_2_f1, step_2_iou = (\n",
    "    evaluate(\n",
    "        model=step_2_model, \n",
    "        test_loader=roof_images_test_loader, \n",
    "        threshold=step_2_threshold, \n",
    "        device=device\n",
    "    )\n",
    ")\n",
    "table_data = [[\"Accuracy\", step_2_accuracy],\n",
    "              [\"F1-Score\", step_2_f1],\n",
    "              [\"IoU\", step_2_iou]]\n",
    "print(tabulate(table_data, headers=[\"Metric\", \"Value\"], tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step n°3 - Training `U-Net` on rotation-augmented, roof-only images\n",
    "\n",
    "During this step, we'll try to improve the performance of our model with some **data augmentation**. This will simulate some **realistic inputs** the model might encounter with new data. We'll be using the library [`albumentations`](https://albumentations.ai), which allows to apply transforms with a probability $p$ on both images and their labels during the training phase.\n",
    "\n",
    "We'll be transforming our original data using a composition of **rotations** (using a uniformly random chosen angle between $0°$, $90°$, $180°$ and $270°$), and **symmetries** around both the $x$-axis and $y$-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "        A.OneOf([\n",
    "                A.Rotate(limit=(0, 0), p=0.25),\n",
    "                A.Rotate(limit=(90, 90), p=0.25),\n",
    "                A.Rotate(limit=(180, 180), p=0.25),\n",
    "                A.Rotate(limit=(270, 270), p=0.25),\n",
    "            ], p=1.0,\n",
    "        ),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "    ],\n",
    "    is_check_shapes=False,\n",
    ")\n",
    "roof_images_train_loader.transform = train_transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same reasoning as above, we provide the commented training code as well as the pre-trained model. Feel free to train the model yourself instead of loading the weights we provide !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_3_model = UNet(nb_layers=nb_layers, kernel_size=kernel_size)\n",
    "if device != \"cuda\":\n",
    "    step_3_model.load_state_dict(torch.load(os.path.join(weights_dir, \"unet_step_1_weights.pth\"), map_location=torch.device('cpu')))\n",
    "else:\n",
    "    step_3_model.load_state_dict(torch.load(os.path.join(weights_dir, \"unet_step_1_weights.pth\")))\n",
    "step_3_model.to(device);\n",
    "\n",
    "# step_3_train_losses, step_3_val_losses = train_model(\n",
    "#     model=step_3_model,\n",
    "#     train_loader=roof_images_train_loader,\n",
    "#     val_loader=roof_images_val_loader,\n",
    "#     device=device,\n",
    "#     criterion=nn.BCELoss(),\n",
    "#     lr=lr,\n",
    "#     epochs=epochs,\n",
    "#     patience=patience\n",
    "# )\n",
    "# plot_losses(step_3_train_losses, step_3_val_losses)\n",
    "\n",
    "step_3_threshold = 0.12\n",
    "# step_3_threshold = optimize_threshold(model=step_3_model, test_loader=roof_images_test_loader, n_trials=20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, we visualize the results !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_image, ground_truth, prediction = predict(\n",
    "    model=step_3_model, \n",
    "    test_loader=roof_images_test_loader,\n",
    "    idx=test_idx, \n",
    "    threshold=step_3_threshold,\n",
    "    device=device,\n",
    ")\n",
    "plot_prediction(real_image, ground_truth, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute our metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_3_accuracy, step_3_f1, step_3_iou = (\n",
    "    evaluate(\n",
    "        model=step_3_model, \n",
    "        test_loader=roof_images_test_loader, \n",
    "        threshold=step_3_threshold,\n",
    "        device=device\n",
    "    )\n",
    ")\n",
    "table_data = [[\"Accuracy\", step_3_accuracy],\n",
    "              [\"F1-Score\", step_3_f1],\n",
    "              [\"IoU\", step_3_iou]]\n",
    "print(tabulate(table_data, headers=[\"Metric\", \"Value\"], tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step n°4 - Training `DeepLabv3` on roof-only images\n",
    "\n",
    "The last step is mainly here for comparison purposes. We would like to try another architecture than `U-Net`, namely [`DeepLabv3`](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/), which was published by Google in 2017 and is known for good results in semantic segmentation tasks like solar panel detection.\n",
    "\n",
    "`DeepLabv3` incorporates so-called [**atrous convolutions**](https://www.analyticsvidhya.com/blog/2023/12/a-comprehensive-guide-on-atrous-convolution-in-cnns/) for capturing multi-scale context and global context awareness. We'll be using constant hyperparameters that we provide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "epochs = 1\n",
    "workers = 2\n",
    "pin_memory = True\n",
    "load_model = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before loading the data, we need to **standardize** the images to ensure consistent input scaling for our Convolutional Neural Network (CNN). Standardization transforms the pixel values so that they have a **mean of $0$** and a **standard deviation of $1$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplab_transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std=[1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")\n",
    "deeplab_train_loader, deeplab_val_loader, deeplab_test_loader = get_loaders(\n",
    "    image_names=image_names[:10],\n",
    "    images_dir=images_dir,\n",
    "    labels_dir=labels_dir,\n",
    "    batch_size=2,\n",
    "    train_transform=deeplab_transform,\n",
    "    val_test_transform=deeplab_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `DiceBCELoss` (see `models/deeplab/utils`) to handle class imbalance, and we'll use **IoU** as our performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Deeplabv3Plus(num_classes=1).to(device)\n",
    "loss_fn = DiceBCELoss()\n",
    "iou_fn = IoU()\n",
    "scaler = torch.amp.GradScaler(str(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train `DeepLab` on the standard images (i.e. with non-roof areas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_iou = train_deeplab(\n",
    "    model=model,\n",
    "    train_loader=deeplab_train_loader,\n",
    "    val_loader=deeplab_val_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    iou_fn=iou_fn,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=epochs,\n",
    "    scaler=scaler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing results\n",
    "\n",
    "For the final comparison of  our models we will be primarily focusing in `F1-score` and `IoU`. Let's start by visualizing these results susing bar plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['U-Net standard', 'U-Net roof-only', 'U-Net augmented roof-only']\n",
    "ious = [step_1_iou, step_2_iou, step_3_iou]\n",
    "f1_scores = [step_1_f1, step_2_f1, step_3_f1]\n",
    "\n",
    "plot_model_metrics(models, ious, f1_scores)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6263587,
     "sourceId": 10146956,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
